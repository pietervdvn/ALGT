
 Related Work and Goals
========================

As _Programming Language Design_ starts to take of as a major field within computer sciences, tools are surfacing which formally define programming languages. All of these tools are created within their own timeframe and goals in mind. In this chapter, we explore what aspects are important for language designers, thus giving the goals of such a tool. Then, we investigate what tools already exist in this space.


Goals and nongoals
------------------

The ultimate goal should be to create a common language for language design, as this would increase formalization of language design. To gain widespread adoption, there should be as little barriers as possible, in installation, usage and documentation. It should be easy for a newcomer to use, without hindering the expressive power or available tools used by the expert.

### Tooling

Practical aspects are important - even the greatest tools lose users over these unecassry barriers.

The first potential barrier is **installation** - which should be as smooth as possible. New users are easily scared by a difficult installation process, fleeing to other tools hindering adoption. 
Preferably, the tool should be availabe in the package repos. If not, installation should be as easy as downloading and running a single binary. Dependencies should be avoided, as these are often hard to deploy on the dev machine - they might be hard to get, to install, having version conflicts with other tools on the machine, not being supported on the operating system of choice...

The second important feature is **documentation**. Documentation should be easy to find, and preferably be distrubeted alongside the binary.

Thirdly, we'll also want to be **cross-platform**.
While most of the PL community uses a Unix-machine, other widely used, non-free operating systems should be supported as well.

As last, extra features like **syntax highlighting**, **automated tests** or having editor support for the target language is a nice touch. Most research tools also offer a **typesetting** module, which give a \LaTeX-version of the language.


### Metalanguage

The most important part is the metalangue itself - as that is the major interface the language designer will use.

As language design itself is already difficult to grasp, we want our language to be **simple and easy**. There are some aspects which help to achieve this.

An easy language should be:

- as **focused** as possible, with little boilerplate. The core concepts of language design should have a central place.
- **expressive** enough to be usefull
- as **simple** as possible, thus having as little elements and special constructs which should be learned and considered when doing automatic translations.
- **checked** as much as possible for big and small errors and report these errors with a clear error message.

#### Embedded in another programming language?

Such a tool can be implemented as library (or domain specific language) embedded in anohter programming language, the host language. The other option is creating a totally new programming language, with a standalone interpreter or compiler.

Implementing the tool as library in a host language gives us a headstart, as all of the builtin functionality and optimazations can be used.
However, the cost later on is high. Starting with a fresh language has quite some benefits:

First, the user does not have to deal with the host language at all. As embedded library, the language designer is forced either learning the new programming language (which takes a lot of time) or ignoring the native bits, and never having a full grasp of his creation. 

Second, by creating a fresh language, its syntax can be streamlined on what is needed: boilerplate can be avoided, making the language more fun to use.

Third, by not using a host language, analysises on metafunctions become possible. Because the metalanguage is small, well understood and explicitly represented in a data structure, it can be modified. This would be hard using a modern host language, as this would involve knowledge of the inner workings of the host language compiler. This is a futile effort, as modern compilers span over 100'000 lines of code, such as the Glasgow Haskell Compiler which has around 140'000 lines of code \cite{GHCSize}. 

As last, we don't have to deal with installing a host compiler, skipping another dependency.

### Parsing the target language

The first step in defining the target language is declaring the syntax, for which the de facto standard has been _BNF_ since its introduction in the ALGOL-report. BNF is well-known to language designers and thus both the theoritical and practical aspects are well understood.
Furthermore, it is easy to port existing languages, as often a BNF is already available for this language. 

A drawback is that many variants of BNF exist, such as EBNF \cite{WirthEBNF}, ABNF \cite{ABNF}, ... Each of these variants have their own extra features, such as constructions for repetition or builtin basic forms.
This is only superficial though: the underlying structure remains the same,  simple search-and-replace can convert one dialect into another.

A new tool should thus use some form BNF to declare the syntax, eventually with small differences which streamline the syntax even more, but with little extra features in order to simplify automatic reasoning.

No other lower-level details should be exposed to the language designer, parsing should be possible only using the BNF. The language designer should not have to deal with tokenization, the parser or the internal definition of the parsetree - this should all be builtin.


### Executing the target language

A major goals is, of course, executing the targat programming language. 
There are multiple ways to do this, which raises the question which one is best suited for prototyping languages. The concerns prototyping languages are:

- **correctness** of the programming language, no matter how the runtime semantics are declared.
- **Immediate and helpfull feedback**: when starting the program, error messages or output should be delivered as soon as possible.
- **Write once, run anywhere**: running target programs should behave the same on all systems.
- **traceable**: it should be possible to see how program execution goes exactly, step by step, as to easily debug.

Explecit _nongoals_ for the tool are:

- running the target program fast. Special efforts optimizing the target language are out of scope, a small and simple implementation is favored.
- compiling executables for any platform. Creating target binaries is not necessary for this research.

This does point in the direction of building an interpreter for the programming language. 

### Typechecking the target language

If desired, a typechecker for the target language should be constructable as well. Preferably, no new syntax should be used to construct this tool. In other words, the typechecker should be build using the same linguistic metaconstructions as the interpreter.

Furthermore, **automatic correctness tests**  should be added, which checks the typechecker in conjunction with interpreter.


Related Work
------------

With these requirements in mind, we investigate what tools already exist and how these tools evolved.


### Yacc

__Yacc__ (Yet Another Compiler Compiler), published in 1975\cite{YACC} was the first tool designed to automatically generate parsers from a given _BNF_-syntax. Depending on the parsed rule, a certain action can be specified - such as constructing a parse tree. 

It was a major step to formally define the syntax of a programming language, but carries a clear legacy of its inception era: you are supposed to include raw `c`-statements, compile to `c` and then compile the generated `c`-code. Furthermore, lexing and parsing are two different steps, requiring two different declarations. Thus, quite some low-level work is needed.

Furthermore, only the parser itself is generated, the parsetree itself should be designed by the language designer.

As this was the first widely available tool for this purpose, it has been tremendously popular, specifically within unix systems, albeit as reimplementation  _GNU bison_. Implementations in other programming languages are widely available.

To modern standards, Yacc is outdated. Of the depicted goals, Yacc can only create a parser based on a given grammer, all the other work is still done by the language designer. Yacc gets an honorable mention for its historical importance, but is not relevant anymore today.


### ANTLR

__ANTLR__ (ANother Tool for Language Recognition) is a more modern _parser generator_\cite{Parr95antlr}. This tool has been widely used as well, as it is compatible with many programming languages, such as Java, C#, Javascript, Python, ...
With this grammer, a parsetree for the input is constructed. Eventually, extra actions can be performed for each part of the parsetree while parsing.

Apart from this first step, the rest of the language design is left to the programmer, which has to work with a chosen host language to build the further steps. This is because ANTLR has a fundamental different goal, which is to create an efficient parser for a language of choice as start of another toolchain.

As example, an implementation for STLC is given in figure \ref{fig:ANTLRExample}


\begin{figure}[h]
\begin{lstlisting}
grammar STFL;

bool	: 'True'
	| 'False';

baseType	: 'Int'
		| 'Bool' ;

typeTerm	: baseType
		| '(' baseType ')';

type	: typeTerm '->' type
	| typeTerm
	;

ID	: [a-z]+ ;

INT	: '0'..'9'+;

value	: bool
	| INT;

e	: eL '+' e
	| eL '::' type 
	| eL e
	| eL ;

eL	: value 
	| ID 
	| '(' '\\' ID ':' type '.' e ')' 
	| 'If' e 'Then' e 'Else' e 
	| '(' e ')';

WS : [\t\r\n ]+ -> skip;
\end{lstlisting}
\caption{The grammar of STLC in ANTLR} 
\label{fig:ANTLRExample}
\end{figure}

### XText

__XText__\cite{XText} is a modern tool to define grammers and associated tooling support. The main use and focus of XText is providing the syntax highlighting, semantic autocompletion, code browsing and other tools featured by the Eclipse IDE. It is thus heavily integrated with java and the Eclipse ecosystem, thus a working knowledge of Java and the Eclipse ecosystem is required - even requiring a working installation of both.

Parsing grammers is done with a metasyntax heavily inspired by ANTLR, enhanced with naming entities and cross references.

In conclusion, XText is a practical tool supporting IDE features, but clearly not suited for the language prototyping we intend to do. 

### LLVM

__LLVM__\cite{LLVM} focusses on the technical aspect of running programs as fast as possible on specific, real world machines. Working with an excellent _intermediate representation_ of imperative programs, LLVM optimizes and compiles target programs to all major computer architectures. 

As it focuses on the backend, _LLVM_ is less suited for easily defining a programming language and thus for researching Language Design. As seen in [their own tutorial](http://llvm.org/docs/tutorial/LangImpl02.html#full-code-listing)\cite{LLVMTutorial}, declaring a parser for a simple programming language takes _nearly 500 lines_ of imperative C-code.

_LLVM_ is thus an industrial strength production tool, made to compile day-to-day programming languages in an efficient way. While it is an extremely usefull piece of software, it's goals are the exact opposite of what we want to achieve.

It would usefull to hook this as backend to _ALGT_, as to further automate the process of creating programming languages. This is however out of scope for this master dissertation.


### MAUDE

__Maude System__\cite{Maude} is a high-level programming language for rewriting and equational logic. It allows a broad range of applications, in a logic-programming driven way. It might be used as a tool to explore the semantics of programming, but it does not meet our needs to easily define programming languages - notably because  overhead is introduced in the tool, both cognitive and syntactic to define even basic languages.

While rewriting rules play a major role in defining semantics, Maude serves as vehicle to experiment with logic and the basics of computation and is less geared toward programming language development.


### PLT-Redex

__PLT-Redex__\cite{PLTRedex} is a DSL implemented in Racket, allowing the declaration of a syntax as BNF and the definition of arbitrary relations, such as reduction or typing. _PLT-Redex_ also features an automated checker, which generates random examples and tests arbitrary properties on them.

As __PLT-Redex__ is a DSL, it assumes knowledge of the host language, _Racket_ . On one hand, it is easy to escape to the host language and use features otherwise not available. On the other hand, this is a practical barrier to designers and hobbyists. A new language has to be learned -Racket is far from popular- and installed, which brings its own problems.

Furthermore, by allowing specification parts to be a full-fledged programming language, it hinders automatic reasoning about several aspects of the definition, which is a major goal in this dissertation.

Thirdly, being a DSL brings syntactic and cognitive overhead of the host language. A fresh programming language, specifically for this task, allows to focus on a clean and to the point syntax.

In other words, _PLT-redex_ is another major step to formally define languages and was a major inspiration to ALGT. However, the approach to embed it within Racket hinders adoption for unexperienced users. 


### OTT

__OTT__ \cite{Sewell07ott:effective} is another major step in the automate formalization of langauges. An OTT-langage is defined in its own format, after which it is translated to either \LaTeX for typesetting or Coq, HOL, Isabelle or Twelf.

Translating the OTT-metalanguage into a proof assistent language has the drawback that knowing such a language is thus a requirement for using OTT. The philosophy of the tool seems to be helping language designers of whom the main tool already is such a language.

By using a new language, the syntax of OTT is more streamlined. However, as the tool translates into a proof assistent language, it still has to make some compromises, such as declaring the datatype a metavariable has.

In conclusion, OTT is another major step to formalization, but has high hurdles for new users. However, both the syntax and concepts of OTT have been an important inspiration.

As example, the grammer definition for STLC is given in figure \ref{fig:ottExample}.


\begin{figure}
\begin{lstlisting}[style=brokenlines]
metavar termvar, x ::=
  {{ isa string }} {{ coq nat }} {{ coq-equality }} {{ hol string }} {{ ocaml int }}
  {{ tex \mathit{[[termvar]]} }} {{ com  term variable  }} 

metavar typvar, X ::=
  {{ isa string }} {{ coq nat }} {{ coq-equality }} {{ hol string }} {{ ocaml int }}
  {{ tex \mathit{[[typvar]]} }} {{ com  type variable  }} 

grammar
  t :: 't_' ::=                                         {{ com term }}
    | x                   ::   :: Var                     {{ com variable }}         
    | \ x . t             ::   :: Lam  (+ bind x in t +)  {{ com abstraction }}      
    | t t'                ::   :: App                     {{ com application }}      
    | ( t )               :: S :: paren   {{ ich [[t]] }} {{ ocaml int }}
    | { t / x } t'        :: M :: tsub    {{ ich ( tsubst_t [[t]] [[x]] [[t']] ) }} {{ ocaml int }}

  v :: 'v_' ::=                                         {{ com  value }}
    | \ x . t             ::   :: Lam                     {{ com abstraction }}

  T :: T_ ::=                                           {{ com type }}
    | X                   ::   :: var                     {{ com variable }}
    | T -> T'             ::   :: arrow                   {{ com function }}
    | ( T )               :: S :: paren {{ ich [[T]] }} {{ ocaml int }}

  G {{ tex \Gamma }} :: G_ ::= {{ isa (termvar*T) list }} {{ coq list (termvar*T) }} {{ ocaml (termvar*T) list }}
                               {{ hol (termvar#T) list }} {{ com type environment }}
    | empty               ::   :: em 
        {{ isa Nil }}
        {{ coq G_nil }}
        {{ hol [] }}
    | G , x : T           ::   :: vn 
        {{ isa ([[x]],[[T]])#[[G]] }}
        {{ coq (cons ([[x]],[[T]]) [[G]]) }}
        {{ hol (([[x]],[[T]])::[[G]]) }}

  terminals :: 'terminals_' ::=
    | \                   ::   :: lambda     {{ tex \lambda }}
    | -->                 ::   :: red        {{ tex \longrightarrow }}
    |  ->                 ::   :: arrow      {{ tex \rightarrow }}
    | |-                  ::   :: turnstile  {{ tex \vdash }}
    | in                  ::   :: in         {{ tex \in }}

\end{lstlisting}
\caption{The grammer definition of a simply typed calculus, declared in OTT. This definition can be downloaded freely from the OTT-site\cite{OTTExample}}
\label{fig:ottExample}
\end{figure}


### The Gradualizer

__The Gradualizer__\cite{Cimini} is a tool designed specifically to create gradual programming languages. It serves as research vehicle, based on the paper by Ronald Garcia, Alison M. Clark and Érik Tanter which proposes an algorithm to gradualize programming languages\cite{GarciaAGT}.

It is a technical tour de force, implementing the entire algorithm described in the AGT-paper in a correct way.

However, it is a specialized tools which only the experts know how to operate. No documentation exists at all, both in the source code or on usage. The input and output format are written in λ-Prolog, which is an unknown language - certainly not suitable for a beginner. The goal of the Gradualizer is to do research specifically on gradualizing certain typesystems automatically.

The gradualizer can handle certain typesystems fully automaticly, at the cost of limiting what typesystems can be gradulaized. The other option is allowing all programming languages at the cost of not being able to fully automate the gradualization.


### ALGT

__ALGT__, which we present in this dissertation, tries to be a generic _compiler front-end_ for arbitrary languages. It should be easy to set up and use, for both hobbyists wanting to create a language and academic researchers trying to create a formally correct language. 

_ALGT_ should handle *all* aspects of Programming Language Design, which is the Syntax, the runtime semantics, the typechecker (if wanted) and the associated properties (such as _Progress_ and _Preservation_) with automatic tests.

By defining runtime semantics, an interpreter is automatically defined and operational as well. This means that no additional effort has to be done to immediatly _run_ a target program.

To maximize ease of use, a build consists of a single binary, containing all that is needed, including the tutorial and Manual.

_ALGT_ is written entirely in Haskell. However, the user of ALGT does not have to leave the _ALGT_-language for any task, so no knowledge of Haskell is needed.

It can be easily extended with additional features. Some of these are already added, such as automatic syntax highlighting, rendering of parsetrees as HTML and LaTeX; but also more advanced features, such as calculation of which syntactic forms are applicable to certain rules or totality and liveability checks of meta functions.



Feature comparison
------------------



In the following table, a comparison of the related work is provided.
The first aspect is **tooling**, which describes the ease of use. This takes into account

- How long _installation_ took and how easy it was.
- If good _documentation_ is easily available.
- If the program runs on _multiple platforms_ (only tested for linux though).
- Whether a _syntax higlighter_ exists for the created language.
- An option to render the typesystem as \LaTeX results in a checkmark for _typesetting_.

Second is the **parsing** aspect, which takes into account:

- _BNF-oriented_ implies that the tool constructs a parser based on a context free grammer, described in BNF or equivalent format.
- _light-syntax_ indicates if the grammer syntax contains little boilerplate and without extra annotations which might confuse an unexperienced reader.
- _parsetree-abstraction_ indicates that the user never has to create a datatype for the parsetree and that the parsetree is automatically constructed.


The **metalanguage** is the language which handles the next steps, such as declaring a reduction rule or a typechecker.

- The metalanguage is _focused_ is parsetrees can be modified with little or no boilerplate.
- The metalanguage is _simple_ if it requires little extra knowlegde, e.g. knowlegde of a host programming language.
- If type-errors are detected in the metalanguage, then it is _typechecked_. This is important, as it prevents construction of malformed parsetrees and other easily preventable errors.

To **execute** the programming language, following aspects are considered:

- _Correctness_ implies that the metalanguage tries to ease reasoning about the semantics
- _Immediate feedback_ meanst that as much usefull feedback is given as soon as possible, such as warnings for possible errors
- _Cross-platform_ execution of the target language is possible, e.g. by having an interpreter available on all major platforms.
- _Debug information or traces_ are usefull when prototyping a new language, to gain insights in how exactly a program execution went.

At last, constructing the **typechecker** should be considered too:

- Preferably, the typechecker can be constructed using the same metaconstructions as the interpreter
- Automatic and/or randomized tests should be performed
- The tools should gradualize or help gradualizing the typesystem

