
 Gradualization of STFL
=========================

In this chapter, gradualization itself is investigated. First, the broad framework of gradual automatic reasoning is painted, from which the advantages of gradual typing follow, together with a small and intuitive example of how gradual typing works in practice, to get some minimal feeling with gradual typing.

Then, using this example, the breakage of preservation is illustrated. This lack of preservation property is used to simplify the runtime proposed by the AGT-paper, yielding smaller and simpler requirements for the runtime.

With the runtime and typechecker requirements in mind, we gradualize STFL twice. The first gradualization is done intuitively and informal to give insight in the requirements, the second time ALGT is used to automate the process as much as possible. 


 Gradual languages
--------------------

A programming language consists of expressions or statements. For these statements, extra information can be calculated using some automatic reasoning tool. Based on the result of this automatic reasoning, some statements could be forbidden, for example when the automatic reasoning deems the statement faulty.

A widely used instance of such automatic reasoning tool is a typechecker. This is the tool which reasons about what type an expression might return. Expressions which combine operators with incompatible operands are brought to the attention of the programmer - often by refusing to compile any further. A lot of programming languages use this technique, such as Haskell, Java, C, ...

Sometimes this automatic reasoning is too strict or too cumbersome. When the reasoning is too strict, advanced constructions with intricate properties might be forbidden while they could be perfectly valid - often the case when extra assumptions are known but can not be modeled within the tool. On the other hand, the automatic reasoning might be too cumbersome to deploy, e.g. when a quick-and-dirty solution is good enough.

Typical languages using the dynamic approach with type checking are Lisp, Prolog, Python, ... Applying an operator on incompatible operands results in a runtime crash.

Both the static and dynamic approach have clear benefits and drawbacks. Using and mixing both throughout the same program would be tremendously usefull: full automatic reasoning within the code for that difficult and intricate algorithm; dynamic behaviour in the glue between components, avoiding unnecessary boilerplate.

In languages with a static typesystem, gradualizing the typesystem means that some parts of the code are statically typechecked, whereas some are not. 
The compiler takes (the lack of) typeannotations as indication to use static or dynamic behaviour: where type annotations are given, the compiler performs the static checks and guarantees that no type errors can occur there. Places in the code without annotations[^typeInference] are treated dynamically, implying that crashes due to type errors can occur there. This also implies that some special care should be given to the borders between the static and dynamic behaviour, to prevent data of a wrong type to enter a statically typed part of the program and cause problems where least expected.

[^typeInference]: Another technique to deal with missing type annotations is _inferencing_ types: the typechecker will calculate what type a certain expression has and silently add the annotations, making typechecking possible again. This is possible and usefull, but out of scope for this dissertation.


In a gradually typed language, the programmer chooses where and how much type annotations are given. This allows not only omit typing when not necessary, but it also allows the migration of codebases. Software projects often start their life as a small proof of concept in a dynamic language, with the intention of being rewritten in a static language when necessary - which'll never happen. The costs of rewriting the project are always deemed to high by management. Gradual typesystems solve this problem: the typing information can be added gradually over time in the codebase. It is possible to start fully dynamic, by not giving any type annotations and move to a totally staticly typed program by giving all type annotations possible.


### Some gradualization of STFL

The essence of a gradual typechecker implementation is **optimism** regarding the missing type information: where typing information is omitted, the typechecker will optimistically assume that the types will work out at runtime. The typing rules are changed to reflect this optimism.

Naturally, before the typing rules can be updated to handle the unknown type, a representation for this missing info is needed. In practice is the missing type information often represented by using `?` as type (instead of `Bool`, `Int` or some function type).

An example expression that passes the gradual typecheck but can not be evaluated, is as following:

	(\ x : ? . x + 1) True

This example should pass the typechecker without problems: the argument `True` is compared to the type `?`. The typechecker optimistically assumes that the function will need a boolean in the end. The typecheck of the function body will use typecheck `x + 1` using `?` for the type of `x`, again optimistically assuming that it will turn out to be an integer at runtime.

Letting the previous example pass the typechecker requires some changes.
For example, the rule which typechecks addition (see figure \ref{fig:additionRep}), should be modified to accomodate the missing type information. This missing information can only come from the terms (`n1` or `n2`) and is only be used to check that both terms are typed as `Int`, with the predicates `n1Type == "Int"` and `n2Type == "Int"`.

If on of these terms turns out to be some other type (such as `Bool`), a type error is reported. If no typing information is known, thus if `n1Type` would be `"?"`, then typechecker should be optimistic and pretend that `n1` is of type `"Int"`.

The rule `[TPlus]` can be gradualized by replacing `==` with an operator doing exactly the comparison above: the operator `~`, named _consistency_. Consistency behaves as `==`, except that it accepts unknown types as well. When confronted with `?`, it is optimistic and holds. The definition of consistency can be found in figure \ref{fig:consistency}.


Swapping out equality `==` for consistency `~` in the typechecking rule will give us the gradual typing counterpart of `[TPlus]`:

\begin{lstlisting}
 Γ ⊢ n1, n1Type	Γ ⊢ n2, n2Type	n1Type ~ "Int"	n2Type ~ "Int"
-------------------------------------------------------------- [TPlus]
 Γ ⊢ n1 "+" n2, "Int"
\end{lstlisting}


This gives some feel of how a gradual typesystem should behave, on an intuitive level. However, some more aspects should be considered as explained in the next part.





\begin{figure}
\begin{lstlisting}
 Γ ⊢ n1, n1Type	Γ ⊢ n2, n2Type	n1Type == "Int"	n2Type == "Int"
-------------------------------------------------------------- [TPlus]
 Γ ⊢ n1 "+" n2, "Int"
\end{lstlisting}
\caption{The typing rule for addition, repeated}
\label{fig:additionRep}
\end{figure}




\begin{figure}
\begin{lstlisting}
 T1 = T2 : type
--------------- [ConsBase]
 T1 ~ T2


 Ta1 ~ Ta2	Tb1 ~ Tb2
---------------------------------- [ConsArrow]
 Ta1 "->" Tb1   ~   Ta2 "->" Tb2


----------	[ConsLeft]
 "?" ~ T2


----------	[ConsRight]
 T1 ~ "?"

\end{lstlisting}
\caption{Natural deduction rules defining consistency}
\label{fig:consistency}
\end{figure}


 Breaking preservation
-----------------------

Using a gradual typesystem breaks one of the essential properties for languages, namely preservation (a definition recap can be found in figure \ref{fig:progPresRecap}).

Preservation breaks on programs containing type inconsistencies which are overlooked by the gradual typesystem. An expression such as `(\x : ? . x + 1) True` passes the gradual typecheck and the reduction becomes `True + 1`, for which no typing rule exists. `True + 1` can _not_ be typed, resulting in preservation not holding anymore.

Progress still holds in this scheme. Any well typed expression is either canonical or can be reduced, because the typing rules mimic the reduction rules; for each typing rule, a respective reduction rule exists which guarantees progress. However, by using gradual typing, a reduction rule might construct a new program which contains a type error, as showcased above.


The reduction relation, constructed for the static language, does not have to deal with type errors at runtime; the preliminary typecheck and preservation properties guarantee that no malformed expression is ever passed into the reduction relation.
At runtime, it is important to detect when preservation breaks, as that is the moment that program execution should be halted with an error. Another aspect that the reduction relation should guarantee is that no crashes can occur in statically typed parts of the program.


\begin{figure}
\begin{lstlisting}

 e0 :: T	e0 → e1
------------------------ [Preservation]
        e1 :: T


 
         e0 :: T
-------------------------- [Progress]
 e0:value    |     e0 → e1 

\end{lstlisting}
\caption{The properties \emph{Preservation} and \emph{Progress}}
\label{fig:progPresRecap}
\end{figure}


